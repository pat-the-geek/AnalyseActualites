# Architecture du projet AnalyseActualitÃ©s

> Documentation technique de l'architecture du systÃ¨me  
> Version 2.0 - 23 janvier 2026

## ğŸ“‹ Table des matiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture logicielle](#architecture-logicielle)
3. [Flux de donnÃ©es](#flux-de-donnÃ©es)
4. [Composants principaux](#composants-principaux)
5. [ModÃ¨le de donnÃ©es](#modÃ¨le-de-donnÃ©es)
6. [IntÃ©grations externes](#intÃ©grations-externes)
7. [Gestion des chemins](#gestion-des-chemins)
8. [SÃ©curitÃ©](#sÃ©curitÃ©)
9. [Performance et scalabilitÃ©](#performance-et-scalabilitÃ©)
10. [DÃ©cisions architecturales](#dÃ©cisions-architecturales)

---

## ğŸ¯ Vue d'ensemble

### Objectif du systÃ¨me
Pipeline automatisÃ© de collecte, traitement et analyse d'articles d'actualitÃ© utilisant l'intelligence artificielle pour gÃ©nÃ©rer des rÃ©sumÃ©s et rapports structurÃ©s.

### Architecture gÃ©nÃ©rale

#### Diagramme ASCII
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Sources RSS/JSON                         â”‚
â”‚              (133 flux d'actualitÃ©s configurÃ©s)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Script de collecte principal                   â”‚
â”‚         (Get_data_from_JSONFile_AskSummary.py)                  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   Collecte   â”‚â†’ â”‚  Extraction  â”‚â†’ â”‚   RÃ©sumÃ© IA  â”‚         â”‚
â”‚  â”‚   HTTP/JSON  â”‚  â”‚     HTML     â”‚  â”‚  (API EurIA) â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Stockage de donnÃ©es                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  data/articles/     â”‚       â”‚   data/raw/         â”‚         â”‚
â”‚  â”‚  (JSON structurÃ©)   â”‚       â”‚  (HTML/texte brut)  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GÃ©nÃ©ration de rapports                        â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚         â”‚  Rapport Markdown synthÃ©tique      â”‚                  â”‚
â”‚         â”‚  (classification par catÃ©gories)   â”‚                  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                          â”‚                                       â”‚
â”‚                          â–¼                                       â”‚
â”‚              rapports/markdown/*.md                              â”‚
â”‚              rapports/pdf/*.pdf                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Diagramme Mermaid interactif

```mermaid
flowchart TB
    %% Sources de donnÃ©es
    RSS["Sources RSS/JSON<br/>(133 flux configurÃ©s)"]
    ENV[".env<br/>Configuration API"]
    CONFIG["config/<br/>sites_actualite.json<br/>categories_actualite.json<br/>prompt-rapport.txt"]
    
    %% Script principal
    MAIN["Get_data_from_JSONFile_AskSummary.py<br/>Script principal ETL"]
    
    %% Ã‰tapes de traitement
    FETCH["Collecte HTTP<br/>RequÃªte vers flux JSON"]
    PARSE["Parsing<br/>Extraction donnÃ©es article"]
    HTML["Extraction HTML<br/>BeautifulSoup4"]
    SUMMARY["GÃ©nÃ©ration rÃ©sumÃ© IA<br/>API EurIA (Qwen3)<br/>Timeout: 60s"]
    IMAGES["Extraction images<br/>Top 3 (largeur > 500px)"]
    
    %% Stockage
    JSON_OUT["data/articles/<br/>articles_generated_YYYY-MM-DD.json"]
    RAW_OUT["data/raw/<br/>all_articles.txt"]
    
    %% GÃ©nÃ©ration rapport
    REPORT["GÃ©nÃ©ration rapport<br/>API EurIA (Qwen3)<br/>Timeout: 300s"]
    MD_OUT["rapports/markdown/<br/>rapport_sommaire_*.md"]
    PDF_OUT["rapports/pdf/<br/>rapport_*.pdf"]
    
    %% Scripts utilitaires
    EXTRACT["Get_htmlText_From_JSONFile.py<br/>Extraction texte brut"]
    CONVERT["articles_json_to_markdown.py<br/>Conversion JSON â†’ MD"]
    
    %% Flux principal
    RSS -->|Lecture| FETCH
    ENV -->|Credentials| MAIN
    CONFIG -->|ParamÃ¨tres| MAIN
    
    MAIN --> FETCH
    FETCH -->|Items JSON| PARSE
    PARSE -->|URLs articles| HTML
    HTML -->|Texte nettoyÃ©| SUMMARY
    HTML -->|Balises img| IMAGES
    
    SUMMARY -->|RÃ©sumÃ© texte| JSON_OUT
    IMAGES -->|MÃ©tadonnÃ©es| JSON_OUT
    
    JSON_OUT -->|Lecture| REPORT
    REPORT -->|Markdown| MD_OUT
    MD_OUT -->|Export| PDF_OUT
    
    %% Flux secondaires
    RSS -->|Lecture| EXTRACT
    EXTRACT -->|Dump texte| RAW_OUT
    
    JSON_OUT -->|Conversion| CONVERT
    CONVERT -->|Formatage| MD_OUT
    
    %% Styling
    classDef sourceStyle fill:#e1f5ff,stroke:#0288d1,stroke-width:2px
    classDef processStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef storageStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef aiStyle fill:#e8f5e9,stroke:#388e3c,stroke-width:3px
    
    class RSS,ENV,CONFIG sourceStyle
    class FETCH,PARSE,HTML,EXTRACT,CONVERT processStyle
    class JSON_OUT,RAW_OUT,MD_OUT,PDF_OUT storageStyle
    class SUMMARY,REPORT,MAIN aiStyle
```

### Principes architecturaux

1. **SÃ©paration des prÃ©occupations** : Scripts, configuration, donnÃ©es et rapports isolÃ©s
2. **Chemins absolus dynamiques** : DÃ©tection automatique de la racine du projet via `__file__`
3. **RÃ©silience** : Retry automatique, gestion d'erreurs exhaustive
4. **TraÃ§abilitÃ©** : Logs horodatÃ©s, mÃ©tadonnÃ©es complÃ¨tes
5. **ExtensibilitÃ©** : Architecture modulaire facilitant l'ajout de nouvelles sources/fonctionnalitÃ©s

---

## ğŸ—ï¸ Architecture logicielle

### Pattern architectural
**Pipeline ETL (Extract, Transform, Load)** avec enrichissement IA

```
Extract          Transform              Load
   â”‚                â”‚                    â”‚
   â”œâ”€ Fetch JSON   â”œâ”€ Parse HTML        â”œâ”€ Save JSON
   â”œâ”€ Parse feeds  â”œâ”€ Summarize (AI)    â”œâ”€ Generate MD
   â””â”€ Get images   â””â”€ Categorize        â””â”€ Export reports
```

### Composants logiciels

#### 1. **Layer de collecte** (`fetch_and_extract_text`)
- ResponsabilitÃ© : RÃ©cupÃ©ration HTTP et parsing HTML
- EntrÃ©e : URL d'article
- Sortie : Texte brut extrait
- DÃ©pendances : `requests`, `beautifulsoup4`

#### 2. **Layer d'enrichissement IA** (`askForResume`, `ask_for_ia`)
- ResponsabilitÃ© : Interaction avec l'API EurIA (Qwen3)
- EntrÃ©e : Texte brut
- Sortie : RÃ©sumÃ© structurÃ© en franÃ§ais
- Features : Retry logic, timeout management, error handling

#### 3. **Layer de traitement d'images** (`extract_top_3_largest_images`)
- ResponsabilitÃ© : Extraction et tri des images pertinentes
- Algorithme : Filtre (largeur > 500px) + tri par surface
- Sortie : Top 3 images avec mÃ©tadonnÃ©es

#### 4. **Layer de persistance**
- Format primaire : JSON structurÃ©
- Format secondaire : Markdown pour rapports
- StratÃ©gie : CrÃ©ation automatique des dossiers si absents

#### 5. **Layer de rapportage** (`create_report`)
- ResponsabilitÃ© : GÃ©nÃ©ration de synthÃ¨ses via IA
- EntrÃ©e : Fichier JSON d'articles
- Sortie : Rapport Markdown structurÃ© par catÃ©gories

---

## ğŸ”„ Flux de donnÃ©es

### Flux principal (collecte et analyse)

```mermaid
flowchart TD
    A[DÃ©marrage du script] --> B{Arguments CLI?}
    B -->|Oui| C[Dates fournies]
    B -->|Non| D[Dates par dÃ©faut: 1er du mois â†’ aujourd'hui]
    C --> E[Chargement .env]
    D --> E
    E --> F[DÃ©tection rÃ©pertoire projet __file__]
    F --> G[CrÃ©ation dossiers si nÃ©cessaires]
    G --> H[Fetch JSON depuis REEDER_JSON_URL]
    H --> I[Extraction items du feed]
    I --> J[Pour chaque item...]
    J --> K[Fetch HTML de l'URL]
    K --> L[Extraction texte avec BS4]
    L --> M[Envoi Ã  API EurIA pour rÃ©sumÃ©]
    M --> N[Extraction top 3 images]
    N --> O[Filtrage par dates]
    O -->|Dans pÃ©riode| P[Ajout Ã  liste data]
    O -->|Hors pÃ©riode| J
    P --> Q{Plus d'items?}
    Q -->|Oui| J
    Q -->|Non| R[Sauvegarde JSON dans data/articles/]
    R --> S[GÃ©nÃ©ration rapport Markdown via IA]
    S --> T[Sauvegarde rapport dans rapports/markdown/]
    T --> U[Fin]
```

### Format des donnÃ©es

#### Flux d'entrÃ©e (RSS/JSON)
```json
{
  "items": [
    {
      "url": "https://source.com/article",
      "date_published": "2026-01-23T10:00:00Z",
      "authors": [{"name": "Auteur"}],
      "title": "Titre de l'article"
    }
  ]
}
```

#### DonnÃ©es intermÃ©diaires (aprÃ¨s extraction)
```python
texts = {
    "https://url1": "Texte extrait de l'article...",
    "https://url2": "Texte extrait de l'article..."
}
```

#### Format de sortie (JSON structurÃ©)
```json
[
  {
    "Date de publication": "2026-01-23T10:00:00Z",
    "Sources": "Nom de la source",
    "URL": "https://...",
    "RÃ©sumÃ©": "RÃ©sumÃ© gÃ©nÃ©rÃ© par l'IA en franÃ§ais...",
    "Images": [
      {
        "url": "https://image.jpg",
        "title": "Titre",
        "alt": "Description",
        "width": 1200,
        "height": 800,
        "area": 960000
      }
    ]
  }
]
```

---

## ğŸ§© Composants principaux

### 1. Get_data_from_JSONFile_AskSummary.py

**RÃ´le** : Script principal de collecte et analyse

**Architecture interne** :
```
Main Program
â”œâ”€â”€ Configuration Loading (load_dotenv)
â”œâ”€â”€ Path Detection & Setup (SCRIPT_DIR, PROJECT_ROOT)
â”œâ”€â”€ Directory Creation (os.makedirs)
â”œâ”€â”€ Data Fetching (requests.get)
â”œâ”€â”€ Processing Loop
â”‚   â”œâ”€â”€ Text Extraction (fetch_and_extract_text)
â”‚   â”œâ”€â”€ AI Summarization (askForResume)
â”‚   â”œâ”€â”€ Image Extraction (extract_top_3_largest_images)
â”‚   â””â”€â”€ Date Filtering (verifier_date_entre)
â”œâ”€â”€ Data Persistence (json.dump)
â””â”€â”€ Report Generation (create_report)
```

**Variables globales** :
- `SCRIPT_DIR` : RÃ©pertoire du script (dÃ©tection via `__file__`)
- `PROJECT_ROOT` : Racine du projet (parent de SCRIPT_DIR)
- `DATA_ARTICLES_DIR` : `{PROJECT_ROOT}/data/articles`
- `DATA_RAW_DIR` : `{PROJECT_ROOT}/data/raw`
- `RAPPORTS_MARKDOWN_DIR` : `{PROJECT_ROOT}/rapports/markdown`
- `URL`, `BEARER` : Credentials API EurIA

**Points d'entrÃ©e** :
```python
# Sans arguments (dates par dÃ©faut)
python Get_data_from_JSONFile_AskSummary.py

# Avec pÃ©riode spÃ©cifique
python Get_data_from_JSONFile_AskSummary.py 2026-01-01 2026-01-31
```

### 2. Get_htmlText_From_JSONFile.py

**RÃ´le** : Extraction texte brut depuis flux JSON

**Flux** :
1. SÃ©lection fichier JSON (GUI tkinter)
2. Parsing et extraction URLs
3. Fetch HTML pour chaque URL
4. Extraction texte avec BeautifulSoup
5. Consolidation dans `all_articles.txt`

**Sortie** : `data/raw/all_articles.txt` (format texte structurÃ©)

### 3. articles_json_to_markdown.py

**RÃ´le** : Conversion JSON â†’ Markdown

**Transformation** :
```
JSON Article â†’ Markdown Block
{                # Date â€” Source
  "Date": "...", â†’ [Lien](URL)
  "URL": "...",  â†’ 
  "RÃ©sumÃ©": "..." â†’ RÃ©sumÃ©
}                â†’ ---
```

---

## ğŸ“Š ModÃ¨le de donnÃ©es

### EntitÃ© principale : Article

| Champ | Type | Source | Description |
|-------|------|--------|-------------|
| `Date de publication` | ISO 8601 String | Flux JSON (`date_published`) | Date originale de publication |
| `Sources` | String | Flux JSON (`authors[0].name`) | Nom de la source/auteur |
| `URL` | URL String | Flux JSON (`url`) | Lien vers l'article original |
| `RÃ©sumÃ©` | Text | API EurIA (gÃ©nÃ©rÃ©) | RÃ©sumÃ© concis en franÃ§ais (max 20 lignes) |
| `Images` | Array[ImageObject] | Extraction HTML (gÃ©nÃ©rÃ©) | Top 3 images par surface |

### Sous-entitÃ© : ImageObject

| Champ | Type | Description |
|-------|------|-------------|
| `url` | URL String | URL absolue de l'image |
| `title` | String | Attribut title de la balise `<img>` |
| `alt` | String | Texte alternatif |
| `width` | Integer | Largeur en pixels |
| `height` | Integer | Hauteur en pixels |
| `area` | Integer | Surface calculÃ©e (width Ã— height) |

### Contraintes mÃ©tier

- **Date** : Format obligatoire `YYYY-MM-DDTHH:MM:SSZ`
- **Images** : Largeur minimale 500px, URLs absolues uniquement
- **RÃ©sumÃ©** : Maximum 20 lignes, langue franÃ§aise
- **PÃ©riode** : `date_debut < date_fin` (validation stricte)

---

## ğŸ”Œ IntÃ©grations externes

### API EurIA (Infomaniak)

**Endpoint** : `https://api.infomaniak.com/euria/v1/chat/completions`

**Authentification** :
```http
Authorization: Bearer {BEARER_TOKEN}
Content-Type: application/json
```

**Payload** :
```json
{
  "messages": [
    {
      "content": "Prompt utilisateur...",
      "role": "user"
    }
  ],
  "model": "qwen3",
  "enable_web_search": true
}
```

**RÃ©ponse** :
```json
{
  "choices": [
    {
      "message": {
        "content": "RÃ©ponse de l'IA..."
      }
    }
  ]
}
```

**Prompts utilisÃ©s dans le projet** :

#### 1. Prompt de rÃ©sumÃ© d'article (fonction `askForResume`)
```
faire un rÃ©sumÃ© de ce texte sur maximum 20 lignes en franÃ§ais, 
ne donne que le rÃ©sumÃ©, sans commentaire ni remarque : {TextToResume}
```

**ParamÃ¨tres** :
- `TextToResume` : Texte HTML extrait de l'article
- Timeout : 60s
- Max attempts : 3

**Objectif** : GÃ©nÃ©rer un rÃ©sumÃ© concis en franÃ§ais de chaque article

#### 2. Prompt de gÃ©nÃ©ration de rapport (fonction `create_report`)
```
Analyse le fichier ce fichier JSON et fait une synthÃ¨se des actualitÃ©s. 
Affiche la date de publication et les sources lorsque tu cites un article. 
Groupe les acticles par catÃ©gories que tu auras identifiÃ©es. 
En fin de synthÃ¨se fait un tableau avec les rÃ©fÃ©rences (date de publication, sources et URL)
pour chaque article dans la rubrique "Images" il y a des liens d'images.
Lorsque cela est possible, publie le lien de l'image sous la forme <img src='{URL}' /> 
sur une nouvelle ligne en fin de paragraphe de catÃ©gorie. N'utilise qu'une image par 
paragraphe et assure-toi qu'une mÃªme URL d'image n'apparaisse qu'une seule fois dans 
tout le rapport.

Filename: {file_output}
File contents:
----- BEGIN FILE CONTENTS -----
{json_str}
----- END FILE CONTENTS -----
```

**ParamÃ¨tres** :
- `file_output` : Chemin du fichier JSON source
- `json_str` : Contenu JSON complet des articles
- Timeout : 300s (5 minutes)
- Max attempts : 3

**Objectif** : CrÃ©er un rapport Markdown structurÃ© avec :
- Classement par catÃ©gories automatique
- Citations avec dates et sources
- Tableau rÃ©capitulatif des rÃ©fÃ©rences
- IntÃ©gration d'images pertinentes

**Gestion des erreurs** :
- **Retry logic** : 3 tentatives par dÃ©faut
- **Timeout** : 60s (rÃ©sumÃ©), 300s (rapport)
- **Fallback** : Message d'erreur standardisÃ©

**Rate limiting** : Non implÃ©mentÃ© (Ã  considÃ©rer pour usage intensif)

---

## ğŸ—‚ï¸ Gestion des chemins

### StratÃ©gie de rÃ©solution (post-refactoring v2.0)

**ProblÃ¨me rÃ©solu** : Scripts fonctionnent indÃ©pendamment du rÃ©pertoire d'exÃ©cution

**Solution** :
```python
# DÃ©tection automatique de la racine du projet
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

# Construction de chemins absolus
DATA_ARTICLES_DIR = os.path.join(PROJECT_ROOT, "data", "articles")
```

**Avantages** :
- âœ… Fonctionne depuis n'importe quel rÃ©pertoire
- âœ… Compatible avec raccourcis macOS
- âœ… Compatible avec cron jobs / automatisation
- âœ… Pas de dÃ©pendance au `cwd` du terminal

### Mapping des chemins

| Constante | Chemin absolu | Usage |
|-----------|---------------|-------|
| `PROJECT_ROOT` | `/Users/.../AnalyseActualitÃ©s` | Racine du projet |
| `SCRIPT_DIR` | `{PROJECT_ROOT}/scripts` | Localisation des scripts |
| `DATA_ARTICLES_DIR` | `{PROJECT_ROOT}/data/articles` | Stockage JSON |
| `DATA_RAW_DIR` | `{PROJECT_ROOT}/data/raw` | DonnÃ©es brutes |
| `RAPPORTS_MARKDOWN_DIR` | `{PROJECT_ROOT}/rapports/markdown` | Rapports gÃ©nÃ©rÃ©s |

---

## ğŸ”’ SÃ©curitÃ©

### Gestion des secrets

**Fichier** : `.env` Ã  la racine du projet

**Variables sensibles** :
```env
bearer=TOKEN_API_CONFIDENTIEL
REEDER_JSON_URL=URL_PRIVEE_DU_FLUX
```

**Protection** :
- âœ… `.env` dans `.gitignore` (jamais versionnÃ©)
- âœ… Chargement via `python-dotenv`
- âœ… Pas de hardcoding des credentials

### Validation des entrÃ©es

**URLs** :
- Validation implicite via `requests.get()` (exceptions levÃ©es)
- VÃ©rification `startswith(('http://', 'https://'))` pour images

**Dates** :
- Parsing strict avec `datetime.strptime()`
- Validation `date_debut < date_fin`

**JSON** :
- Try/except sur `json.load()` et `response.json()`

### Gestion des erreurs rÃ©seau

```python
try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
except requests.exceptions.HTTPError:
    # HTTP 4xx / 5xx
except requests.exceptions.ConnectionError:
    # Pas de connexion
except requests.exceptions.Timeout:
    # Timeout dÃ©passÃ©
except requests.exceptions.RequestException:
    # Autres erreurs rÃ©seau
```

---

## âš¡ Performance et scalabilitÃ©

### Goulots d'Ã©tranglement actuels

1. **SÃ©quentiel** : Traitement article par article (pas de parallÃ©lisation)
2. **RÃ©seau** : Latence cumulÃ©e des requÃªtes HTTP (fetch + API IA)
3. **API IA** : Temps de gÃ©nÃ©ration ~5-10s par rÃ©sumÃ©

### MÃ©triques estimÃ©es

| OpÃ©ration | Temps moyen | Commentaire |
|-----------|-------------|-------------|
| Fetch HTML | 1-3s | DÃ©pend de la source |
| Extraction texte | <100ms | TrÃ¨s rapide (local) |
| RÃ©sumÃ© IA | 5-10s | API externe (variable) |
| Extraction images | 1-2s | Fetch + parsing |
| **Total par article** | **7-15s** | Sans parallÃ©lisation |

**Pour 100 articles** : ~12-25 minutes

### Optimisations possibles

#### Court terme
1. **Filtrage anticipÃ©** : Filtrer par date AVANT extraction
   ```python
   # TODO dÃ©jÃ  notÃ© dans le code
   items_filtered = [i for i in items if date_in_range(i['date_published'])]
   ```

2. **Cache HTTP** : Utiliser `requests-cache` pour Ã©viter re-fetch

3. **Batch AI requests** : Grouper plusieurs rÃ©sumÃ©s en une requÃªte

#### Moyen terme
1. **ParallÃ©lisation** : `concurrent.futures` ou `asyncio`
   ```python
   with ThreadPoolExecutor(max_workers=5) as executor:
       texts = executor.map(fetch_and_extract_text, urls)
   ```

2. **Queue system** : Redis + Celery pour traitement asynchrone

3. **Incremental processing** : Ne traiter que les nouveaux articles

#### Long terme
1. **Base de donnÃ©es** : Migration JSON â†’ PostgreSQL/SQLite
2. **Cache distribuÃ©** : Redis pour textes extraits
3. **Microservices** : SÃ©parer collecte / enrichissement IA / rapports

---

## ğŸ¯ DÃ©cisions architecturales

### ADR-001 : Chemins absolus vs relatifs

**Contexte** : Scripts v1.0 utilisaient chemins relatifs (`../data/`), causaient erreurs avec raccourcis macOS

**DÃ©cision** : DÃ©tection automatique via `__file__` + construction chemins absolus

**ConsÃ©quences** :
- âœ… Fonctionne depuis n'importe quel rÃ©pertoire
- âœ… Compatible automatisation
- âš ï¸ LÃ©gÃ¨rement plus verbeux

### ADR-002 : JSON comme format de stockage primaire

**Alternatives considÃ©rÃ©es** : CSV, SQLite, PostgreSQL

**DÃ©cision** : JSON structurÃ©

**Justification** :
- Simple Ã  manipuler (natif Python)
- Lisible par humains
- Compatible avec la plupart des outils
- Pas de setup de DB requis

**Limites** :
- âŒ Pas de requÃªtes complexes
- âŒ Pas de relations entre entitÃ©s
- âŒ Performance limitÃ©e pour gros volumes

### ADR-003 : RÃ©sumÃ©s IA en franÃ§ais uniquement

**DÃ©cision** : Forcer langue franÃ§aise dans les prompts

**Justification** :
- Sources principalement francophones
- Utilisateur francophone
- CohÃ©rence des rapports

**Impact** : Limite rÃ©utilisabilitÃ© pour sources non-franÃ§aises

### ADR-004 : Retry automatique sans backoff exponentiel

**ImplÃ©mentation actuelle** :
```python
for attempt in range(max_attempts):
    try:
        response = requests.post(...)
        # ...
    except:
        continue  # Retry immÃ©diat
```

**Risque** : Peut surcharger API en cas d'erreur systÃ©mique

**AmÃ©lioration future** : ImplÃ©menter backoff exponentiel
```python
time.sleep(2 ** attempt)  # 2s, 4s, 8s...
```

### ADR-005 : Interface GUI (tkinter) pour sÃ©lection fichiers

**Contexte** : Scripts `Get_htmlText_From_JSONFile.py` et `articles_json_to_markdown.py` utilisent dialogs

**Avantages** :
- âœ… User-friendly
- âœ… Pas d'arguments CLI Ã  mÃ©moriser

**InconvÃ©nients** :
- âŒ Incompatible environnements headless
- âŒ Bloque automatisation complÃ¨te

**Alternative** : Ajouter support arguments CLI optionnels

---

## ğŸ“ˆ MÃ©triques et monitoring

### Logs actuels

**Format** :
```
YYYY-MM-DD HH:MM:SS Message
```

**ImplÃ©mentation** :
```python
def print_console(msg: str):
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} {msg}")
```

**AmÃ©liorations suggÃ©rÃ©es** :
1. Niveau de log (DEBUG/INFO/WARNING/ERROR)
2. Output vers fichier en plus de stdout
3. Structured logging (JSON)

### MÃ©triques Ã  suivre

| MÃ©trique | Comment la calculer | UtilitÃ© |
|----------|-------------------|---------|
| Articles traitÃ©s/jour | Count dans JSON | Tendance volume |
| Temps moyen/article | Timestamps dÃ©but/fin | Optimisation |
| Taux d'erreur extraction | Failed / Total | QualitÃ© sources |
| Taux d'erreur API IA | Retry count | FiabilitÃ© API |
| Taille rapports gÃ©nÃ©rÃ©s | File size | Monitoring stockage |

---

## ğŸ”® Ã‰volutions futures

### Roadmap technique

#### Phase 1 : Stabilisation (Q1 2026)
- [ ] Tests unitaires (pytest)
- [ ] CI/CD avec GitHub Actions
- [ ] Documentation API (Sphinx)
- [ ] Backoff exponentiel pour retry

#### Phase 2 : Performance (Q2 2026)
- [ ] ParallÃ©lisation avec asyncio
- [ ] Cache HTTP (requests-cache)
- [ ] Filtrage anticipÃ© par dates

#### Phase 3 : ScalabilitÃ© (Q3 2026)
- [ ] Migration vers PostgreSQL
- [ ] Queue system (Celery + Redis)
- [ ] API REST pour exposer les donnÃ©es

#### Phase 4 : Features (Q4 2026)
- [ ] DÃ©tection automatique de catÃ©gories
- [ ] Analyse de sentiment
- [ ] GÃ©nÃ©ration graphiques/visualisations
- [ ] Export multi-formats (PDF, EPUB)

---

## ğŸ“š RÃ©fÃ©rences

### Documentation externe
- [BeautifulSoup4 Docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Requests Docs](https://requests.readthedocs.io/)
- [Python-dotenv](https://github.com/theskumar/python-dotenv)
- [API EurIA Infomaniak](https://euria.infomaniak.com)

### Standards suivis
- [PEP 8](https://peps.python.org/pep-0008/) â€“ Style Guide for Python Code
- [ISO 8601](https://www.iso.org/iso-8601-date-and-time-format.html) â€“ Date and time format
- [Semantic Versioning](https://semver.org/) â€“ Versioning scheme

---

**Document maintenu par** : Patrick Ostertag  
**DerniÃ¨re mise Ã  jour** : 23 janvier 2026  
**Version** : 2.0
